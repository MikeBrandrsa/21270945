---
output:
  md_document:
    variant: markdown_github
---
Financial Econometrics Practical

# Outline

This Readme will outline all the following questions by means of explaining, interpreting and reasoning through all the necessary code and functions, whereafter a simplified output will be reproduced within each respective question folders.

First, I sanitize my working environment and source all the necessary functions that will be incorporated into our analyses.
```{r, include=FALSE}

rm(list = ls())
gc()
library(tidyverse)
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
```

```{r, include=FALSE}
# this is a commented out chunk indicating the process by which the questions folders where produced.

# HTML Texevier:
#Texevier::create_template_html(directory = "/Users/michaelbrand/OneDrive/Documents/UNI/2021/2/Fin.Metrics/Test/21270945/Questions",
 #                         template_name = "Question1"
#)
#Texevier::create_template_html(directory = "/Users/michaelbrand/OneDrive/Documents/UNI/2021/2/Fin.Metrics/Test/21270945/Questions",
#                         template_name = "Question2"
#)
#Texevier::create_template_html(directory = "/Users/michaelbrand/OneDrive/Documents/UNI/2021/2/Fin.Metrics/Test/21270945/Questions",
#                         template_name = "Question3"
#)
#Texevier::create_template_html(directory = "/Users/michaelbrand/OneDrive/Documents/UNI/2021/2/Fin.Metrics/Test/21270945/Questions",
#                          template_name = "Question4"
#)
#Texevier::create_template_html(directory = "/Users/michaelbrand/OneDrive/Documents/UNI/2021/2/Fin.Metrics/Test/21270945/Questions",
#                          template_name = "Question5"
#)
#Texevier::create_template_html(directory = "/Users/michaelbrand/OneDrive/Documents/UNI/2021/2/Fin.Metrics/Test/21270945/Questions",
#                          template_name = "Question6"
#)
```
Now I procceed with the respective questions.

# Question 1: Yield Spread.

Since the beginning of 2020 the current Yield spreads in local mid to longer dated bond yields have been the highest in decades. This is conventionally expressed as the difference in these yields of these instruments in percentage points or basis points.

```{r, include= FALSE}
library(tidyverse)
SA_bonds <- read_rds("data/SA_Bonds.rds")
#BE_Infl <- read_rds("data/BE_Infl.rds")
#bonds_2y <- read_rds("data/bonds_2y.rds")
#bonds_10y <- read_rds("data/bonds_10y.rds")
#usdzar <- read_rds("data/usdzar.rds")
#ZA_Infl <- read_rds("data/ZA_Infl.rds")
#IV <- read_rds("data/IV.rds")

```

```{r, echo=FALSE}
library(tbl2xts)
library(PerformanceAnalytics)

bond_Adj <- 
SA_bonds %>% gather(Bond, Yield, -date) %>% arrange(date) 

bondplot1 <- bond_Adj %>% 
ggplot() + 
geom_line(aes(x = date, y = Yield, color = Bond), alpha = 0.8, 
    size = 1.2)

bondplot1 <- bondplot1 + fmxdat::theme_fmx() + theme(legend.position = "bottom") + labs(x = "date", 
    y = "Yield", title = "Yield of ZAR Bonds.", subtitle = "Using 3 Month, 2 Year and 10 year bond yields.", caption = "Note:\nNico Katzke's data used")

print(bondplot1)

```
we notice several places where rates stagnate and thus check for missing values in the data.
```{r, echo=FALSE, include=FALSE}
is.na(bond_Adj)
sum(is.na(bond_Adj))
```
it appears that all the data is present.

Now at first glance an immediate divergence can be observed between the three bond yields, specifically with the three-month and two-year yields diverging from the longer 10 year bond yield. Before investigating this from an economic and quantitative perspective it is worth investigating it more formally.
```{r}
bond_Adj_yield <- SA_bonds %>%
    arrange(date) %>% 
mutate("10Yr2Yr_spread" = ZA_10Yr - ZA_2Yr,"10Yr3M_spread" = ZA_10Yr - SA_3M,"2Yr3M_spread" = ZA_2Yr - SA_3M ) %>% select(date, "10Yr3M_spread","10Yr2Yr_spread","2Yr3M_spread" )

bond_Adj_yield_tdy <- bond_Adj_yield %>% gather(bondpair, Spread , -date) %>% arrange(date) 

bondplot2 <- bond_Adj_yield_tdy %>% ggplot() + geom_line(aes(x = date, y = Spread, color = bondpair), alpha = 0.8, size = 1.2) 

bondplot2 <- bondplot2 + fmxdat::theme_fmx() + theme(legend.position = "bottom") + labs(x = "date", 
    y = "Yield Spread", title = "Yield Spread of ZAR Bonds.", subtitle = "Using 3 Month, 2 Year and 10 year bond yields.", caption = "Note:\nNico Katzke's data used")

print(bondplot2)
```

```{r}
bond_Adj_yield_tdy2010 <- bond_Adj_yield %>% gather(bondpair, Spread , -date) %>% arrange(date) %>% 
    filter(date > "2010-01-01")

bondplot3 <- bond_Adj_yield_tdy2010 %>% ggplot() + geom_line(aes(x = date, y = Spread, color = bondpair), alpha = 0.8, size = 1.2) 

bondplot3 <- bondplot3 + fmxdat::theme_fmx() + theme(legend.position = "bottom") + labs(x = "date", 
    y = "Yield Spread", title = "Yield Spread of ZAR Bonds.", subtitle = "Using 3 Month, 2 Year and 10 year bond yields.", caption = "Note:\nNico Katzke's data used")

print(bondplot3)
```
\newpage

# Question 2: Portfolio Construction

```{r, include=FALSE}
T40 <- read_rds("data/T40.rds")
RebDays <- read_rds("data/Rebalance_days.rds")
``` 

```{r}
library(PerformanceAnalytics)
library(tbl2xts)

# effrtnsQ2 <- T40 %>% arrange(date) %>% gather(Tickers, Return, -date) %>% 
   # tbl2xts::tbl_xts(cols_to_xts = Return, spread_by = Tickers) %>% 
   # PerformanceAnalytics::Return.calculate(., method = "log") %>% 
   # tbl2xts::xts_tbl()

# porteqw <- rmsfuns::Safe_Return.portfolio(Rtn, weight = Wt, geometric = FALSE)
### use this ####
    ####    ####
```




```{r}

# j400j200effectivereturns <- T40 %>% select(date, Tickers, Return, J400, J200) %>% 
  #  arrange(date) %>% mutate(effj200rtn = Return*J200) %>% 
  #  mutate(effj400rtn = Return*J400) %>% select(date,effj200rtn,effj400rtn) 

#Cum_JJRtn <- j400j200effectivereturns %>% arrange(date) %>% 
  #  mutate(CumJ2Rtn = (cumprod(1 + effj200rtn))) %>% mutate(CumJ4Rtn = (cumprod(1 + effj400rtn)))# Start at 1)
```


# Question 3: Volatility Comparison

get return series for just j200
```{r}
#pacman::p_load("tidyverse", "devtools", "FactoMineR", "factoextra", 
   # "broom", "rmsfuns")
#T40 <- read_rds("data/T40.rds")
#Q2ALSIRTN <- T40  %>% arrange(date) %>% select(date, Tickers, Return, J200) %>% drop_na()
#colSums(is.na(Q3ALSIRTN))
#WghtALSIRTN <- Q3ALSIRTN %>% mutate(effectivereturn = Return*J200) %>% 
 #   group_by(date) %>% summarise(portfolio_return = sum(effectivereturn)) %>% ungroup()
 # this is for Q2 

```

```{r}
pacman::p_load("tidyverse", "devtools", "FactoMineR", "factoextra", 
    "broom", "rmsfuns")
T40 <- read_rds("data/T40.rds")
Q3ALSIRTN <- T40  %>% arrange(date) %>% select(date, Tickers, Return, J200) %>% drop_na() %>% select(date, Tickers, Return) %>% mutate(Return = ifelse(Return > 0.25, 0.25, 
    ifelse(Return < -0.25, -0.25, Return)))
colSums(is.na(Q3ALSIRTN))
```

```{r}
Q3plot1 <- Q3ALSIRTN %>% ggplot() + geom_line(aes(date, Return, color=Tickers, alpha =0.9)) +
                                                   labs(x = "date", y = "Returns", title = "top40 ALSI Returns", subtitle = "", caption = "Note:\nNico Katzke's data used")
Q3plot1
```

```{r}

Q3centered <- Q3ALSIRTN %>% group_by(Tickers) %>% mutate(Return = Return - mean(Return)) %>% ungroup() %>% spread(Tickers, Return)


set.seed(1234)
        NAll <- nrow(Q3centered %>% gather(Return, Returns, -date))

Q3centered <- bind_cols(
          Q3centered %>% gather(Tickers, Return, -date),
          Q3centered %>% gather(Tickers, Return, -date) %>%
            mutate(Dens = list(density(Return, na.rm=T))) %>%
            summarise(Random_Draws = list(sample(Dens[[1]]$x, NAll, replace = TRUE, prob=.$Dens[[1]]$y))) %>% 
            unnest(Random_Draws)) %>% mutate(Return = coalesce(Return, Random_Draws)) %>% select(-Random_Draws) %>% spread(Tickers, Return)
return(Q3centered)
any(is.na(Q3centered))
```
\newpage
```{r}
Q3centemax <- Q3centered %>% select(-date)
Q3covmat <- cov(Q3centemax)  
```

```{r}
# eigenvectors:
evec <- eigen(Q3covmat, symmetric = TRUE)$vector
# eigenvalues:
eval <- eigen(Q3covmat, symmetric = TRUE)$values

lambda = diag(t(evec) %*% Q3covmat %*% evec)
# Which should be equal to eval:
all.equal(lambda, eval)

prcomp(Q3covmat)
prop = eval/sum(eval)
print(prop)
```

```{r}
prop <- tibble(Loadings = prop) %>% mutate(PC = paste0("PC_", 
    row_number()))

prop[, "PC"][[1]] <- factor(prop[, "PC"][[1]], levels = prop$PC)
Prop2 <- prop %>% slice_head(n=10)
g <- Prop2 %>% 
ggplot() + geom_bar(aes(PC, Loadings), stat = "identity", fill = "steelblue") + 
    
fmxdat::theme_fmx(axis.size.title = fmxdat::ggpts(38), axis.size = fmxdat::ggpts(35), 
    title.size = fmxdat::ggpts(42), CustomCaption = T) + 
scale_y_continuous(breaks = scales::pretty_breaks(10), labels = scales::percent_format(accuracy = 1)) + 
labs(x = "Principal Components", y = "Loadings", title = "Eigenvalue proportions", 
    caption = "Source: Fmxdat Package")
g
```


```{r}
#pcaseries <- Q3ALSIRTN %>% spread(Tickers, Return) %>% select(-date)
#demean = scale(pcaseries, center = TRUE, scale = TRUE)


#Q3ALSIRTN

#sigma <- cov(demean )

#evec <- eigen(sigma, symmetric = TRUE)$vector  #eigen vectors
#eval <- eigen(sigma, symmetric = TRUE)$values  #eigen values

# or done automagically (using standard covariance matrix
# warts and all):
#pcrun <- prcomp(demean, center = T, scale. = T)
#ev <- pcrun$rotation

#e_min1 <- solve(ev)  # Invert eigenvector as in equation above)
#R_PCA <- e_min1 %*% t(pcaseries)  # Note: pcaseries, not demean.

#PC_Series <- t(R_PCA) %>% tibble::as_tibble()

#PC_Series <- PC_Series %>% mutate(date = unique(Spots$date)) %>% 
    #gather(Type, Val, -date)

#---- Let's look at these series:

# Returns:
#PC_Series %>% filter(Type %in% paste0("PC", 1:5)) %>% ggplot() + 
    #geom_line(aes(date, Val, color = Type))



```
now we add the supplementary variables.
```{r}
library(PerformanceAnalytics)
library(tbl2xts)
rf <-read_rds("data/SA_Bonds.rds") %>% 
    select(date, SA_3M) %>%  gather(Bond, Yield, -date) %>%
    arrange(date)
rfxts <- tbl_xts(rf)

xtsQ3ALSIRTN <- tbl_xts(Q3ALSIRTN)
chart.RollingCorrelation(Ra = xtsQ3ALSIRTN , Rb=rfxts, width = 100, xaxis = TRUE,
  legend.loc = NULL, colorset = (1:12), fill = NA)

```


# Question 4: Volatility and GARCH estimates


```{r}
cncy <- read_rds("data/currencies.rds")
cncy_Carry <- read_rds("data/cncy_Carry.rds")
cncy_value <- read_rds("data/cncy_value.rds")
cncyIV <- read_rds("data/cncyIV.rds")
bbdxy <- read_rds("data/bbdxy.rds")
PPP <- read_csv("data/ppp.csv", col_names = T ) 
```

vol compared to other currencies or are they all vol.

```{r}
Q4cncieszar <- cncy %>% spread(Name, Price) %>% select(date, SouthAfrica_Cncy) %>% gather(Spot, Price, -date) %>% mutate(Spot = gsub("_Cncy", "", Spot)) %>%
    mutate(Spot = gsub("SouthAfrica", "USD/ZAR", Spot))

Q4cnciesplot <- Q4cncieszar %>% ggplot() +  geom_line(aes(date, Price, color = Spot)) + fmxdat::theme_fmx() + labs(x = "date", 
    y = "Spot Price", title = "USDZAR Spot Rate.", subtitle = "Changes in the strength of the ZAR overtime.", caption = "Note:\nNico Katzke's data used")
Q4cnciesplot
```

```{r}
# Q4value <- cncy_value %>%  spread(Name, Price)
#%>%  select(date, SouthAfrica_Cncy) %>% gather(Spot, Price, -date) %>% mutate(Spot = gsub("_Cncy", "", Spot)) %>%
   # mutate(Spot = gsub("SouthAfrica", "USD/ZAR", Spot))

```

```{r}
library(lubridate)
Q4PPP <- PPP %>% select(TIME, LOCATION, Value) %>% spread(LOCATION, Value) %>% 
    select(TIME, ZAF, USA) %>% gather(Spot, Price, -TIME)# %>%  mutate(YearMonth = format(date, "%Y%B"))

Q4cnciesplot2 <- Q4PPP %>% ggplot() +  geom_line(aes(TIME, Price, color = Spot)) + fmxdat::theme_fmx() + labs(x = "date", 
    y = "Spot Price", title = "USDZAR Spot Rate.", subtitle = "Changes in the strength of the ZAR overtime.", caption = "Note:\nNico Katzke's data used")
Q4cnciesplot2
```

```{r}
library(tbl2xts)

Q4dlogzar <- Q4cncieszar %>% mutate(Price = na.locf(Price)) %>% arrange(date)  %>% mutate(dlogret = log(Price) - log(lag(Price))) %>% mutate(scaledret = (dlogret -  mean(dlogret,na.rm =T))) %>% filter(date > dplyr::first(date))
    
Q4plot3 <- Q4dlogzar %>% ggplot() + geom_line(aes(x = date, y = scaledret, colour = Spot, 
    alpha = 0.5)) + 
ggtitle("Log-Scaled USD/ZAR Returns") + 
guides(alpha = "none") + 
fmxdat::theme_fmx()

ggplot(Q4dlogzar) + 
geom_histogram(aes(x = scaledret, fill = Spot, alpha = 0.5))
```

definite potential for bias due to clumping (auto-persitance in returns) so use perfrimanceana, to clean

```{r}
Rtn <-  Q4dlogzar %>% # Easier to work with ymd here 
   tbl_xts(., cols_to_xts = dlogret, spread_by = Spot)

Rtn[is.na(Rtn)] <- 0

Plotdata = cbind(Rtn, Rtn^2, abs(Rtn))

colnames(Plotdata) = c("Returns", "Returns_Sqd", "Returns_Abs")

Plotdata <- 
Plotdata %>% xts_tbl() %>% 
gather(ReturnType, Returns, -date)

ggplot(Plotdata) + 
geom_line(aes(x = date, y = Returns, colour = ReturnType, alpha = 0.5)) + 
    
ggtitle("Return Type Persistence: USD/ZAR") + 
facet_wrap(~ReturnType, nrow = 3, ncol = 1, scales = "free") + 
    
guides(alpha = "none", colour = "none") + 
fmxdat::theme_fmx()
```
From the above figure it seems that:

There remains strong first order persistence in returns
There is clearly periods of strong second order persistence
There is clear evidence of long memory in the second order process.

```{r}
forecast::Acf(Rtn, main = "ACF: Equally Weighted Return")
forecast::Acf(Rtn^2, main = "ACF: Squared Equally Weighted Return")
forecast::Acf(abs(Rtn), main = "ACF: Absolute Equally Weighted Return")
```
The above proves what we expected - in particular very strong conditional heteroskedasticity, as well as long memory.

A formal test for ARCH effects: LBQ stats on squared returns:
```{r}
Box.test(coredata(Rtn^2), type = "Ljung-Box", lag = 12)

```
The test rejects the nulls of no ARCH effects - hence we need to control for the remaining conditional heteroskedasticity in the returns series,


```{r}
library(rugarch)
garch11 <- 
  
  ugarchspec(
    
    variance.model = list(model = c("sGARCH","gjrGARCH","eGARCH","fGARCH","apARCH")[1], 
                          
    garchOrder = c(1, 1)), 
    
    mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), 
    
    distribution.model = c("norm", "snorm", "std", "sstd", "ged", "sged", "nig", "ghyp", "jsu")[1])

# Now to fit, I use as.matrix and the data - this way the plot functions we will use later will work.

garchfit1 = ugarchfit(spec = garch11,data = Rtn) 

# Note it saved a S4 class object - having its own plots and functionalities:
class(garchfit1)

```

```{r, results='asis'}

library(xtable)
slotNames(garchfit1)
names(garchfit1@fit)
names(garchfit1@model)

# Use it now as follows:
garchfit1@fit$matcoef  # Model coefficients.

# Include it in your paper as follows:
pacman::p_load(xtable)
Table <- xtable(garchfit1@fit$matcoef)
print(Table, type = "latex", comment = FALSE)
```

```{r}
persistence(garchfit1)
```

```{r}
sigma <- sigma(garchfit1) %>% xts_tbl() 
colnames(sigma) <- c("date", "sigma") 
sigma <- sigma %>% mutate(date = as.Date(date))

gg <- 
  
ggplot() + 
  geom_line(data = Plotdata %>% filter(ReturnType == "Returns_Sqd") %>% select(date, Returns) %>% 
              
              unique %>% mutate(Returns = sqrt(Returns)), aes(x = date, y = Returns)) + 
  
  geom_line(data = sigma, aes(x = date, y = sigma), color = "red", size = 2, alpha = 0.8) + 
  
  # scale_y_continuous(limits = c(0, 0.35)) + 
  labs(title = "Comparison: Returns Sigma vs Sigma from Garch", 
       
       subtitle = "Note the smoothing effect of garch, as noise is controlled for.", x = "", y = "Comparison of estimated volatility",
       
       caption = "Source: Fin metrics class | Calculations: Own") + 
  
    fmxdat::theme_fmx(CustomCaption = TRUE)


fmxdat::finplot(gg, y.pct = T, y.pct_acc = 1)


```

```{r}
ni <- newsimpact(z = NULL, garchfit1)

plot(ni$zx, ni$zy, ylab = ni$yexpr, xlab = ni$xexpr, type = "l", 
    main = "News Impact Curve")
```

```{r}
sigma <- sigma(garchfit1)  # Conditional resids

epsilon <- residuals(garchfit1)  # Ordinary resids

zt <- residuals(garchfit1, standardize = TRUE)  # Standardized resids

# Check the class notes on the definitions of epsilon and zt:
ztbyhand = epsilon/sigma

# And they are exactly the same:
all.equal(zt, ztbyhand)
infocriteria(garchfit1)
plot(garchfit1, which = 1)
```

```{r}
plot(garchfit1, which = 9)
plot(garchfit1, which = 8)
```

```{r}
plot(garchfit1, which = 3)

```
 Note for the above plot - the blue line is sigma, gray line
 epsilon (i.e. sigma + noise.): notice the massive
 distorting impact of noise in ordinary epsilon, or squared
 error / S.D. measures: which are typically used for
 proxying vol.


```{r}
gjrgarch11 = ugarchspec(variance.model = list(model = c("sGARCH","gjrGARCH","eGARCH","fGARCH","apARCH")[2], 
                                              
                                              garchOrder = c(1, 1)), 
                        
                        mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), 
                        
                        distribution.model = c("norm", "snorm", "std", "sstd", "ged", "sged", "nig", "ghyp", "jsu")[3])
# Now to fit, I use as.matrix and the data - this way the plot functions we will use later will work.
garchfit2 = ugarchfit(spec = gjrgarch11, data = as.matrix(Rtn)) 

garchfit2@fit$matcoef %>% xtable()
```


```{r}


```

# Question 5: MSCI Funds 

```{r}
msci <- read_rds("data/msci.rds")
bonds <- read_rds("data/bonds_10y.rds")
comms <- read_rds("data/comms.rds")

```
Definition of MSCI series:

These are daily Total Return series for MSCI funds. The names are self-explanatory, with MSCI_RE being the global real estate fund, MSCI_USREIT being the US real estate fund, and MSCI_ACWI the MSCI All Country World Index.

```{r}
library(lubridate)
Q5MSCI <-msci %>% spread(Name, Price) %>% select(date, MSCI_ACWI, MSCI_RE)

Q5bond <- bonds %>% spread(Name, Bond_10Yr) %>% select(date, US_10Yr)

Q5Com <- comms %>%  spread(Name, Price) %>% select(date, Bcom_Index)

Q5Diverspotential <- Q5MSCI %>% left_join( Q5bond, by = c("date")) %>% 
    left_join( Q5Com, by = c("date")) %>% gather(Tickers, Price, -date) %>%
    group_by(Tickers) %>%
    filter(date > as.Date("2000-01-01")) %>% arrange(date) %>%  
    group_by(Tickers) %>%  mutate(dlogret = log(Price) - log(lag(Price))) %>% mutate(scaledret = (dlogret -  mean(dlogret, na.rm = T))) %>% filter(date > dplyr::first(date)) %>%  ungroup()
```

```{r}
pacman::p_load("MTS", "robustbase")
pacman::p_load("tidyverse", "devtools", "rugarch", "rmgarch", 
    "forecast", "tbl2xts", "lubridate", "PerformanceAnalytics", 
    "ggthemes")
Q5xts_rtn <- Q5Diverspotential %>% tbl_xts(., cols_to_xts = "dlogret", spread_by = "Tickers")
MarchTest(Q5xts_rtn)
DCCPre <- dccPre(Q5xts_rtn, include.mean = T, p = 0)
```


```{r}
Vol <- DCCPre$marVol
colnames(Vol) <- colnames(Q5xts_rtn)
Vol <- 
  data.frame( cbind( date = index(Q5xts_rtn), Vol)) %>% # Add date column which dropped away...
  mutate(date = as.Date(date)) %>%  tbl_df()  # make date column a date column...
TidyVol <- Vol %>% gather(Stocks, Sigma, -date)
ggplot(TidyVol) + geom_line(aes(x = date, y = Sigma, colour = Stocks))
```

```{r}
StdRes <- DCCPre$sresi

cl = makePSOCKcluster(10)
pacman::p_load("tidyverse", "tbl2xts", "broom")

uspec <- ugarchspec(
    variance.model = list(model = "gjrGARCH", 
    garchOrder = c(1, 1)), 
    mean.model = list(armaOrder = c(1, 
    0), include.mean = TRUE), 
    distribution.model = "sstd")

multi_univ_garch_spec <- multispec(replicate(ncol(Q5xts_rtn), uspec))

spec.go <- gogarchspec(multi_univ_garch_spec, 
                       distribution.model = 'mvnorm', # or manig.
                       ica = 'fastica') # Note: we use the fastICA
cl <- makePSOCKcluster(10)

multf <- multifit(multi_univ_garch_spec, Q5xts_rtn, cluster = cl)

fit.gogarch <- gogarchfit(spec.go, 
                      data = Q5xts_rtn, 
                      solver = 'hybrid', 
                      cluster = cl, 
                      gfun = 'tanh', 
                      maxiter1 = 40000, 
                      epsilon = 1e-08, 
                      rseed = 100)

print(fit.gogarch)
```

```{r}
gog.time.var.cor <- rcor(fit.gogarch)
gog.time.var.cor <- aperm(gog.time.var.cor,c(3,2,1))
dim(gog.time.var.cor) <- c(nrow(gog.time.var.cor), ncol(gog.time.var.cor)^2)
# Finally:
```

```{r}
renamingdcc <- function(ReturnSeries, DCC.TV.Cor) {
  
ncolrtn <- ncol(ReturnSeries)
namesrtn <- colnames(ReturnSeries)
paste(namesrtn, collapse = "_")

nam <- c()
xx <- mapply(rep, times = ncolrtn:1, x = namesrtn)
# Now let's be creative in designing a nested for loop to save the names corresponding to the columns of interest.. 

# TIP: draw what you want to achieve on a paper first. Then apply code.

# See if you can do this on your own first.. Then check vs my solution:

nam <- c()
for (j in 1:(ncolrtn)) {
for (i in 1:(ncolrtn)) {
  nam[(i + (j-1)*(ncolrtn))] <- paste(xx[[j]][1], xx[[i]][1], sep="_")
}
}

colnames(DCC.TV.Cor) <- nam

# So to plot all the time-varying correlations wrt SBK:
 # First append the date column that has (again) been removed...
DCC.TV.Cor <- 
    data.frame( cbind( date = index(ReturnSeries), DCC.TV.Cor)) %>% # Add date column which dropped away...
    mutate(date = as.Date(date)) %>%  tbl_df() 

DCC.TV.Cor <- DCC.TV.Cor %>% gather(Pairs, Rho, -date)

DCC.TV.Cor

}
gog.time.var.cor <- renamingdcc(ReturnSeries = Q5xts_rtn, DCC.TV.Cor = gog.time.var.cor)

```



```{r}
Q5compplot <- ggplot(gog.time.var.cor %>% filter(grepl("Bcom_", Pairs), 
    !grepl("_Bcom", Pairs))) + geom_line(aes(x = date, y = Rho, 
    colour = Pairs)) + theme_hc() + ggtitle("Go-GARCH: Bcom")

print(Q5compplot)
```

```{r}
Q5bondplot <- ggplot(gog.time.var.cor %>% filter(grepl("US_10Yr", Pairs), 
    !grepl("_US", Pairs))) + geom_line(aes(x = date, y = Rho, 
    colour = Pairs)) + theme_hc() + ggtitle("Go-GARCH: US_10Yr")

print(Q5bondplot)
```

```{r}
Q5ACWIplot <- ggplot(gog.time.var.cor %>% filter(grepl("MSCI_ACWI", Pairs), 
    !grepl("_MSCI_ACWI", Pairs))) + geom_line(aes(x = date, y = Rho, 
    colour = Pairs)) + theme_hc() + ggtitle("Go-GARCH: ACWI")

print(Q5ACWIplot)
```

```{r}
Q5REplot <- ggplot(gog.time.var.cor %>% filter(grepl("MSCI_RE", Pairs), 
    !grepl("_MSCI_RE", Pairs))) + geom_line(aes(x = date, y = Rho, 
    colour = Pairs)) + theme_hc() + ggtitle("Go-GARCH: RE")

print(Q5ACWIplot)
```

```{r}
library(cowplot)
plot_grid(Q5bondplot, Q5ACWIplot, Q5REplot , Q5compplot, labels = c('', '', '',''))
```

```{r}
library(factoextra)
library(FactoMineR)
Q5PCAdata <- Q5Diverspotential %>%  select(date, Tickers, dlogret)%>% spread(Tickers, 
    dlogret) %>% select(-date)

Q5PCA <- prcomp(Q5PCAdata, center = TRUE, scale. = TRUE)
Q5PCA$rotation
plot(Q5PCA, type = "l")
summary(Q5PCA)
pacman::p_load("psych")
pairs.panels(Q5PCAdata)

gviolion <- Q5PCAdata %>% gather(Type, val) %>% ggplot() + geom_violin(aes(Type, 
    val, fill = Type), alpha = 0.7) + fmxdat::theme_fmx() + fmxdat::fmx_fills()

fmxdat::finplot(gviolion, y.pct = T, y.pct_acc = 1, x.vert = T)
```

Note: This is a very useful figure to include in your reports on the behaviour of returns data as it shows similarity in distribution and density between the returns.

